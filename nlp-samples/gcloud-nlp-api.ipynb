{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Natural Language API\n",
    "\n",
    "- [Natural Language API Client Libraries](https://cloud.google.com/natural-language/docs/reference/libraries#client-libraries-usage-python)\n",
    "- [Google Cloud Natural Language API Python Samples](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/language/cloud-client/v1)\n",
    "\n",
    "First, authenticate by running the following command in an interactive terminal:\n",
    "```\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = \"\"\"Hello, world!\n",
    "President Obama is speaking at the White House.\n",
    "Ladies and gentlemen!\n",
    "世界、こんにちは！\n",
    "今日は天気いいですね！\n",
    "悲しいニュースですね\n",
    "素晴らしい\n",
    "Google Cloud Natural Language API は、使いやすい REST API を介して強力な機械学習モデルを提供することで、テキストの構造と意味を解析できるようにします。この API を使用すれば、ドキュメント、ニュース記事、ブログ記事に含まれる人、場所、イベントなどに関する情報を抽出できるようになります。ソーシャル メディア上のコメントから商品に対するセンチメント（感情）を把握したり、コールセンターやメッセージ アプリに寄せられた消費者の意見から顧客満足度を分析したりすることができます。リクエストでアップロードしたテキストを分析することも、Google Cloud Storage のドキュメント ストレージ上のデータを分析することもできます。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Text: Hello, world!\n",
      "Score: 0.4\n",
      "Magnitude: 0.4\n",
      "====================\n",
      "name            : world\n",
      "type            : LOCATION\n",
      "metadata        : {}\n",
      "salience        : 1\n",
      "wikipedia_url   : -\n",
      "================================================================================\n",
      "X: Hello\n",
      "PUNCT: ,\n",
      "NOUN: world\n",
      "PUNCT: !\n",
      "====================================================================================================\n",
      "Text: President Obama is speaking at the White House.\n",
      "Score: 0.2\n",
      "Magnitude: 0.2\n",
      "====================\n",
      "name            : Obama\n",
      "type            : PERSON\n",
      "metadata        : {'mid': '/m/02mjmr', 'wikipedia_url': 'http://en.wikipedia.org/wiki/Barack_Obama'}\n",
      "salience        : 0.9077594\n",
      "wikipedia_url   : http://en.wikipedia.org/wiki/Barack_Obama\n",
      "====================\n",
      "name            : White House\n",
      "type            : LOCATION\n",
      "metadata        : {'mid': '/m/081sq', 'wikipedia_url': 'http://en.wikipedia.org/wiki/White_House'}\n",
      "salience        : 0.092240565\n",
      "wikipedia_url   : http://en.wikipedia.org/wiki/White_House\n",
      "================================================================================\n",
      "NOUN: President\n",
      "NOUN: Obama\n",
      "VERB: is\n",
      "VERB: speaking\n",
      "ADP: at\n",
      "DET: the\n",
      "NOUN: White\n",
      "NOUN: House\n",
      "PUNCT: .\n",
      "====================================================================================================\n",
      "Text: Ladies and gentlemen!\n",
      "Score: 0.3\n",
      "Magnitude: 0.3\n",
      "====================\n",
      "name            : Ladies\n",
      "type            : PERSON\n",
      "metadata        : {}\n",
      "salience        : 0.72030365\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : gentlemen\n",
      "type            : PERSON\n",
      "metadata        : {}\n",
      "salience        : 0.27969635\n",
      "wikipedia_url   : -\n",
      "================================================================================\n",
      "NOUN: Ladies\n",
      "CONJ: and\n",
      "NOUN: gentlemen\n",
      "PUNCT: !\n",
      "====================================================================================================\n",
      "Text: 世界、こんにちは！\n",
      "Score: 0.4\n",
      "Magnitude: 0.4\n",
      "====================\n",
      "name            : 世界\n",
      "type            : LOCATION\n",
      "metadata        : {}\n",
      "salience        : 1\n",
      "wikipedia_url   : -\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/language/cloud-client/v1/snippets.py\n",
    "import argparse\n",
    "\n",
    "from google.cloud import language\n",
    "import six\n",
    "\n",
    "\n",
    "def sentiment_text(text):\n",
    "    \"\"\"Detects sentiment in the text.\"\"\"\n",
    "    language_client = language.Client()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = language_client.document_from_text(text)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.doc_type == language.Document.HTML\n",
    "    sentiment = document.analyze_sentiment().sentiment\n",
    "    \n",
    "    print('Score: {}'.format(sentiment.score))\n",
    "    print('Magnitude: {}'.format(sentiment.magnitude))\n",
    "\n",
    "\n",
    "def sentiment_file(gcs_uri):\n",
    "    \"\"\"Detects sentiment in the file located in Google Cloud Storage.\"\"\"\n",
    "    language_client = language.Client()\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = language_client.document_from_url(gcs_uri)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.doc_type == language.Document.HTML\n",
    "    sentiment = document.analyze_sentiment().sentiment\n",
    "\n",
    "    print('Score: {}'.format(sentiment.score))\n",
    "    print('Magnitude: {}'.format(sentiment.magnitude))\n",
    "\n",
    "\n",
    "def entities_text(text):\n",
    "    \"\"\"Detects entities in the text.\"\"\"\n",
    "    language_client = language.Client()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = language_client.document_from_text(text)\n",
    "\n",
    "    # Detects entities in the document. You can also analyze HTML with:\n",
    "    #   document.doc_type == language.Document.HTML\n",
    "    entities = document.analyze_entities().entities\n",
    "\n",
    "    for entity in entities:\n",
    "        print('=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', entity.name))\n",
    "        print(u'{:<16}: {}'.format('type', entity.entity_type))\n",
    "        print(u'{:<16}: {}'.format('metadata', entity.metadata))\n",
    "        print(u'{:<16}: {}'.format('salience', entity.salience))\n",
    "        print(u'{:<16}: {}'.format('wikipedia_url',\n",
    "              entity.metadata.get('wikipedia_url', '-')))\n",
    "\n",
    "\n",
    "def entities_file(gcs_uri):\n",
    "    \"\"\"Detects entities in the file located in Google Cloud Storage.\"\"\"\n",
    "    language_client = language.Client()\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = language_client.document_from_url(gcs_uri)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.doc_type == language.Document.HTML\n",
    "    entities = document.analyze_entities().entities\n",
    "\n",
    "    for entity in entities:\n",
    "        print('=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', entity.name))\n",
    "        print(u'{:<16}: {}'.format('type', entity.entity_type))\n",
    "        print(u'{:<16}: {}'.format('metadata', entity.metadata))\n",
    "        print(u'{:<16}: {}'.format('salience', entity.salience))\n",
    "        print(u'{:<16}: {}'.format('wikipedia_url',\n",
    "              entity.metadata.get('wikipedia_url', '-')))\n",
    "\n",
    "\n",
    "def syntax_text(text):\n",
    "    \"\"\"Detects syntax in the text.\"\"\"\n",
    "    language_client = language.Client()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = language_client.document_from_text(text)\n",
    "\n",
    "    # Detects syntax in the document. You can also analyze HTML with:\n",
    "    #   document.doc_type == language.Document.HTML\n",
    "    tokens = document.analyze_syntax().tokens\n",
    "\n",
    "    for token in tokens:\n",
    "        print(u'{}: {}'.format(token.part_of_speech, token.text_content))\n",
    "\n",
    "\n",
    "def syntax_file(gcs_uri):\n",
    "    \"\"\"Detects syntax in the file located in Google Cloud Storage.\"\"\"\n",
    "    language_client = language.Client()\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = language_client.document_from_url(gcs_uri)\n",
    "\n",
    "    # Detects syntax in the document. You can also analyze HTML with:\n",
    "    #   document.doc_type == language.Document.HTML\n",
    "    tokens = document.analyze_syntax().tokens\n",
    "\n",
    "    for token in tokens:\n",
    "        print(u'{}: {}'.format(token.part_of_speech, token.text_content))\n",
    "#\n",
    "for text in texts.split('\\n'):\n",
    "    print('====================================================================================================')\n",
    "    print('Text: {}'.format(text))\n",
    "    sentiment_text(text)\n",
    "    entities_text(text)\n",
    "    print('================================================================================')\n",
    "    syntax_text(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
